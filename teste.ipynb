{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5],\n",
      "        [ 0],\n",
      "        [ 6],\n",
      "        [ 9],\n",
      "        [13],\n",
      "        [ 2],\n",
      "        [ 0],\n",
      "        [14],\n",
      "        [ 9],\n",
      "        [12]])\n",
      "______\n",
      "tensor([5])\n",
      "______\n",
      "tensor([[ 5],\n",
      "        [ 0],\n",
      "        [ 6],\n",
      "        [ 9],\n",
      "        [13],\n",
      "        [ 2],\n",
      "        [ 0],\n",
      "        [14],\n",
      "        [ 9],\n",
      "        [12]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "t() expects a tensor with <= 2 dimensions, but self is 3D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 82\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39m# Gerar letras para um exemplo do dataframe\u001b[39;00m\n\u001b[1;32m     81\u001b[0m start_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mi love you\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 82\u001b[0m generated_lyrics \u001b[39m=\u001b[39m generate_lyrics(start_text, num_verses\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     83\u001b[0m \u001b[39mprint\u001b[39m(generated_lyrics)\n",
      "Cell \u001b[0;32mIn[19], line 65\u001b[0m, in \u001b[0;36mgenerate_lyrics\u001b[0;34m(start_text, num_verses, max_length)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     64\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_length):\n\u001b[0;32m---> 65\u001b[0m         output \u001b[39m=\u001b[39m model(input_tensor)\n\u001b[1;32m     66\u001b[0m         probabilities \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(output[\u001b[39m0\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     67\u001b[0m         predicted_index \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoices(\u001b[39mrange\u001b[39m(n_vocab), weights\u001b[39m=\u001b[39mprobabilities)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 38\u001b[0m, in \u001b[0;36mSimple_LSTM.forward\u001b[0;34m(self, seq_in)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, seq_in):\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mprint\u001b[39m(seq_in\u001b[39m.\u001b[39;49mt())\n\u001b[1;32m     39\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m______\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m     \u001b[39mprint\u001b[39m(seq_in\u001b[39m.\u001b[39mt()[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: t() expects a tensor with <= 2 dimensions, but self is 3D"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "# Exemplo simulado de um dataframe com letras\n",
    "data = {'letra': [\"i love you\", \"you're the one\", \"in the moonlight\"]}\n",
    "lyrics_df = pd.DataFrame(data)\n",
    "\n",
    "# Pré-processamento das letras\n",
    "all_lyrics = \" \".join(lyrics_df['letra'])  # Concatenar todas as letras\n",
    "all_lyrics = all_lyrics.lower()  # Converter para minúsculas\n",
    "\n",
    "# Criar um dicionário de caracteres únicos presentes nas letras\n",
    "chars = sorted(list(set(all_lyrics)))\n",
    "char_to_index = {char: index for index, char in enumerate(chars)}\n",
    "index_to_char = {index: char for index, char in enumerate(chars)}\n",
    "\n",
    "# Parâmetros do modelo\n",
    "n_vocab = len(chars)\n",
    "hidden_dim = 128\n",
    "embedding_dim = 64\n",
    "dropout = 0.2\n",
    "seq_length = 1  # Agora estamos gerando um caractere de cada vez\n",
    "\n",
    "# Definir a classe Simple_LSTM\n",
    "class Simple_LSTM(nn.Module):\n",
    "    def __init__(self, n_vocab, hidden_dim, embedding_dim, dropout=0.2):\n",
    "        super(Simple_LSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, dropout=dropout, num_layers=2)  # Definindo a camada LSTM\n",
    "        self.embeddings = nn.Embedding(n_vocab, embedding_dim)  # Camada de embeddings\n",
    "        self.fc = nn.Linear(hidden_dim, n_vocab)  # Camada totalmente conectada para a saída\n",
    "    \n",
    "    def forward(self, seq_in):\n",
    "        embedded = self.embeddings(seq_in)  # Obter representações de embedding diretamente\n",
    "        lstm_out, _ = self.lstm(embedded.permute(1, 0, 2))  # Passar pelas camadas LSTM com permutação\n",
    "        ht = lstm_out[-1]  # Pegar a saída da última etapa da LSTM\n",
    "        out = self.fc(ht)  # Passar pela camada totalmente conectada para obter a saída\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Simple_LSTM(n_vocab, hidden_dim, embedding_dim, dropout)\n",
    "\n",
    "# Função para gerar letras\n",
    "def generate_lyrics(start_text, num_verses=3, max_length=1000):\n",
    "    model.eval()\n",
    "\n",
    "    start_text = start_text.lower()\n",
    "    \n",
    "    input_sequence = [char_to_index[char] for char in start_text]\n",
    "    input_tensor = torch.unsqueeze(torch.tensor(input_sequence), 0)\n",
    "    \n",
    "    generated_text = start_text\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            output = model(input_tensor)\n",
    "            probabilities = torch.nn.functional.softmax(output[0], dim=0).numpy()\n",
    "            predicted_index = random.choices(range(n_vocab), weights=probabilities)[0]\n",
    "            predicted_char = index_to_char[predicted_index]\n",
    "            \n",
    "            generated_text += predicted_char\n",
    "            input_tensor = torch.unsqueeze(torch.tensor([[predicted_index]]), 0)\n",
    "            \n",
    "            if predicted_char == '\\n':\n",
    "                num_verses -= 1\n",
    "                if num_verses <= 0:\n",
    "                    break\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Gerar letras para um exemplo do dataframe\n",
    "start_text = \"i love you\"\n",
    "generated_lyrics = generate_lyrics(start_text, num_verses=2)\n",
    "print(generated_lyrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outinho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
